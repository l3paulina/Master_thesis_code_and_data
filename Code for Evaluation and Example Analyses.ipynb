{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e2f38b-187c-4c6d-9d18-744fcf9e9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcb47bd9-c330-444b-a58e-f1d072db25dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the new dataset with annotated reproducibility scores\n",
    "file_path = \"New Annotated Dataset.xlsx\" \n",
    "sheet_index = 2  #Third sheet from the Excel file\n",
    "\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dccf4784-8e55-4ad6-b5db-6b3d8292c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable definitions for easier computation\n",
    "factors = ['Data availability', 'Inclusion of a ReadMe', 'Trained model inclusion', 'Hyperparameter description', 'Training description',\n",
    "    'Paper readability', 'Code availability'] #Reproducibility criteria\n",
    "\n",
    "annotated_col = 'The reproducibility factor from the dataset by Olszewski et al.' #The annotated reproducibility factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d64b5dc-20b1-4b19-b1ec-890bc76b064f",
   "metadata": {},
   "source": [
    "Evaluation of the Created Quantitative Reproducibility Measure for Machine Learning Research Papers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c4a8c-a521-458e-8d1a-ba73c0894aff",
   "metadata": {},
   "source": [
    "Normalization of reproducibility criteria scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74599ee-cf8d-45d3-b1d2-0da8a8a0d30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the reproducibility criteria scores with min-max\n",
    "df_norm = df.copy() #Copy to not change the original data\n",
    "\n",
    "#Looping through the criteria\n",
    "for col in factors:\n",
    "    min_val = df_norm[col].min() #Minimum value\n",
    "    max_val = df_norm[col].max() #Maximum value\n",
    "    df_norm[col] = (df_norm[col] - min_val) / (max_val - min_val) #Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714d0a5-0809-41ff-87d1-1a7deb90e818",
   "metadata": {},
   "source": [
    "Calculating the reproducibility score with WSM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9b403ac-05b7-426a-ad04-01d01868019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the reproducibility score with WSM. Without the weight selection for the reproducibility criteria, the calculation\n",
    "#is just a mean.\n",
    "\n",
    "df_norm['WSM_score'] = df_norm[factors].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc3946-e9cf-4664-8c4d-5455154a4870",
   "metadata": {},
   "source": [
    "Applying a threshold for a research paper to be classified as 0 or 1. Because the derived reproducibility scores with WSM were in the interval [0,1], a 0.5 threshold was selected to classify a paper as reproducible (value = 1). This was necessary as the reproducibility factor, from the dataset by D. Olszewski, et al., was provided as a binary variable containing only values of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6767bfd3-a8aa-4b22-8b1c-e2d6620db2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a column for the 0 and 1 labels derived from the WSM reproducibility calculation\n",
    "df_norm['WSM_reproducible'] = (df_norm['WSM_score'] > 0.5).astype(int) \n",
    "\n",
    "#the 0.5 was chosen from a manual sensitivity analysis by increasing the threshold levels from 0 to 1, by 0.1 increments. \n",
    "#0.5 provided the most optimal solution with the highest level of accuracy, precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733428f1-65c1-4e81-8c83-7f0a0747651c",
   "metadata": {},
   "source": [
    "Comparing the reproducibility score with WSM and the reproducibility factor, from the dataset by D. Olszewski, et al.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0100f13-e384-4c23-b3ca-18df2421bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copying the annotated reproducibility column in a dataframe, for easier calculation later\n",
    "df_norm['Annotated_reproducible'] = df_norm[annotated_col]\n",
    "\n",
    "#Creating a new column 'Match' that checks if WSM prediction matches the annotated reproducibility factor, for easier calculation later\n",
    "df_norm['Match'] = (df_norm['WSM_reproducible'] == df_norm['Annotated_reproducible'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baaf630c-d722-4aec-9a5f-29a651d337a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 139\n",
      "Correct predictions: 89\n",
      "Accuracy: 0.6403\n"
     ]
    }
   ],
   "source": [
    "#Calculating total papers, correctly predicted number of papers and accuracy\n",
    "\n",
    "total_papers = len(df_norm) #Total number of papers\n",
    "correct = df_norm['Match'].sum() #Sum of paper with matching results\n",
    "accuracy = correct / total_papers\n",
    "\n",
    "#Displaying results\n",
    "print(\"Total papers:\", total_papers)\n",
    "print(\"Correct predictions:\", correct)\n",
    "print(\"Accuracy:\", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e10ab469-6cb0-43a7-b9ba-bd7da378e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "WSM         0   1\n",
      "Annotated        \n",
      "0          63  39\n",
      "1          11  26\n"
     ]
    }
   ],
   "source": [
    "#Creating a confusion matrix to see results per reproducible and not reproducible paper type\n",
    "confusion_matrix = pd.crosstab(\n",
    "    df_norm['Annotated_reproducible'],\n",
    "    df_norm['WSM_reproducible'],\n",
    "    rownames=['Annotated'],\n",
    "    colnames=['WSM']\n",
    ")\n",
    "#Displaying results\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1914ece9-b24d-4cb9-9ea2-4c92291c6d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4\n",
      "Recall: 0.7027\n",
      "F1-score: 0.5098\n"
     ]
    }
   ],
   "source": [
    "#Calculating precision, recall and f-1\n",
    "y_true = df_norm['Annotated_reproducible'] #True value labels from the reproducibility factor\n",
    "y_pred = df_norm['WSM_reproducible'] #The labels derived from the WSM reproducibility calculation \n",
    "\n",
    "precision = precision_score(y_true, y_pred) #precision\n",
    "recall = recall_score(y_true, y_pred) #recall\n",
    "f1 = f1_score(y_true, y_pred) #f-1\n",
    "\n",
    "#Displaying results\n",
    "print(\"Precision:\", round(precision, 4))\n",
    "print(\"Recall:\", round(recall, 4))\n",
    "print(\"F1-score:\", round(f1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56053164-1998-452e-b220-3651e2e2a623",
   "metadata": {},
   "source": [
    "Example Analysis of the Quantitative Reproducibility Measure for Machine Learning Research Papers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e6455-0c67-4773-8d8d-91868d625edc",
   "metadata": {},
   "source": [
    "Criteria evaluation with the Saaty's importance scale for three different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "582312ad-749c-46b7-b6fa-ebf259976fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scenario importance score vectors for the three weight scenarios\n",
    "scenario_scores = {\n",
    "    \"Scenario_1_equal\": np.array([1, 1, 1, 1, 1, 1, 1], dtype=float), #Equal weights scenario\n",
    "    \"Scenario_2_data_code\": np.array([7, 3, 3, 3, 3, 3, 7], dtype=float), #Data and code focused scenario\n",
    "    \"Scenario_3_read_train\": np.array([3, 3, 1, 4, 7, 7, 3], dtype=float), #Readability and training focused scenario\n",
    "}\n",
    "\n",
    "RI = 1.32  #Random Index for n=7 (n=7 because there are 7 criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52236a-6fa7-4d0f-a40d-5deba3dc20bc",
   "metadata": {},
   "source": [
    "Functions to create a pairwise comparison matrix equivalent and for the consistency ratio evaluation for the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bfcb72b-5f71-4743-acd7-4d08c690101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting a vector of scores into normalized weights that sum to 1\n",
    "\n",
    "def scores_to_weights(scores): #Normalizing raw scores so that their sum equals 1\n",
    "    return scores / scores.sum()\n",
    "\n",
    "def build_pairwise_matrix(scores): #Building a pairwise comparison matrix from a score vector. Each element A[i, j] = scores[i] / scores[j]\n",
    "    return scores[:, None] / scores[None, :]\n",
    "\n",
    "#Get AHP priority weights from the pairwise comparison matrix\n",
    "def ahp_weights(A):\n",
    "    A_norm = A / A.sum(axis=0) #Normalization of each column of the matrix\n",
    "    return A_norm.mean(axis=1) #Mean of each row to obtain the weight vector\n",
    "\n",
    "#Consistency metrics for the AHP pairwise comparison matrix\n",
    "def consistency_ratio(A, weights, RI): #Pairwise comparison matrix, AHP-derived weight vector, Random index for matrices of size n\n",
    "    n = A.shape[0]\n",
    "    lambda_max = np.mean((A @ weights) / weights) #Maximum eigenvalue\n",
    "    CI = (lambda_max - n) / (n - 1) #Consistency index\n",
    "    CR = CI / RI #Consistency ratio\n",
    "    return lambda_max, CI, CR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949cc093-268e-45ed-8eaa-390f843dae45",
   "metadata": {},
   "source": [
    "Consistency index for the weights in weight scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d86d33d0-ccbe-4280-baed-53ae29ed2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Scenario  lambda_max   CI   CR\n",
      "0       Scenario_1_equal         7.0  0.0  0.0\n",
      "1   Scenario_2_data_code         7.0  0.0  0.0\n",
      "2  Scenario_3_read_train         7.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "#Creating a list for results for each scenario\n",
    "results = []\n",
    "\n",
    "#Loop through each scenario and it's score vector\n",
    "for name, scores in scenario_scores.items():\n",
    "    A = build_pairwise_matrix(scores) #Creating a AHP pairwise comparison matrix from the score vector\n",
    "    w = ahp_weights(A) #AHP priority weights from the pairwise matrix\n",
    "    lambda_max, CI, CR = consistency_ratio(A, w, RI) #Consistency metrics\n",
    "    results.append([name, lambda_max, CI, CR])\n",
    "\n",
    "#Converting results into a dataframe for easier results vizualization\n",
    "cr_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Scenario\", \"lambda_max\", \"CI\", \"CR\"]\n",
    ")\n",
    "#Displaying results\n",
    "print(cr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13099f02-12dc-4fbd-90f3-0974a2b221ae",
   "metadata": {},
   "source": [
    "Normalizing weights for each weight scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5a4637f-870d-4aa7-97c5-25ec4b3b2514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scenario weights:\n",
      "                            Scenario_1_equal  Scenario_2_data_code  \\\n",
      "Data availability                   0.142857              0.241379   \n",
      "Inclusion of a ReadMe               0.142857              0.103448   \n",
      "Trained model inclusion             0.142857              0.103448   \n",
      "Hyperparameter description          0.142857              0.103448   \n",
      "Training description                0.142857              0.103448   \n",
      "Paper readability                   0.142857              0.103448   \n",
      "Code availability                   0.142857              0.241379   \n",
      "\n",
      "                            Scenario_3_read_train  \n",
      "Data availability                        0.107143  \n",
      "Inclusion of a ReadMe                    0.107143  \n",
      "Trained model inclusion                  0.035714  \n",
      "Hyperparameter description               0.142857  \n",
      "Training description                     0.250000  \n",
      "Paper readability                        0.250000  \n",
      "Code availability                        0.107143  \n",
      "\n",
      "Column sums:\n",
      "Scenario_1_equal         1.0\n",
      "Scenario_2_data_code     1.0\n",
      "Scenario_3_read_train    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Creating normalized weights for each scenario\n",
    "\n",
    "weights = {name: scores_to_weights(s) for name, s in scenario_scores.items()} #Each score vector is converted into weights that sum to 1\n",
    "weights_df = pd.DataFrame(weights, index=factors) #Converting the weights dictionary into a dataframe for easier application in next parts of the code\n",
    "\n",
    "#Displaying results\n",
    "print(\"\\nScenario weights:\")\n",
    "print(weights_df)\n",
    "print(\"\\nColumn sums:\")\n",
    "print(weights_df.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862fc547-3440-4f74-9b09-77717b635b0e",
   "metadata": {},
   "source": [
    "Computing WSM scores for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51991fb0-35a5-426a-9202-606db37b15de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looping through each scenario and its corresponding weight vector\n",
    "for name, w in weights.items():\n",
    "    df_norm[f\"WSM_{name}\"] = df_norm[factors].values @ w \n",
    "    \n",
    "#Computing the WSM score for each row: WSM = sum of (criteria value Ã— corresponding weight). The @ operator performs matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55574ac-1196-4a52-8b06-7aa0594dd68e",
   "metadata": {},
   "source": [
    "Randomly sample 10 papers from the new annotated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ccfd1b8-c8e8-4796-a13b-342bb4183d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Paper number                                        Paper Title  \\\n",
      "0           271  WTAGRAPH: Web Tracking and Advertising Detecti...   \n",
      "1           461  DeepSteal: Advanced Model Extractions Leveragi...   \n",
      "2           269  Model Stealing Attacks Against Inductive Graph...   \n",
      "3           613  Phishpedia: A Hybrid Deep Learning Based Appro...   \n",
      "4           552  Cheetah: Lean and Fast Secure Two-Party Deep N...   \n",
      "5           459  DEEPCASE: Semi-Supervised Contextual Analysis ...   \n",
      "6           556  Khaleesi: Breaker of Advertising and Tracking ...   \n",
      "7           257  AI/ML for Network Security: The Emperor has no...   \n",
      "8           740  Improving Password Guessing via Representation...   \n",
      "9            80  Privacy Risks of Securing Machine Learning Mod...   \n",
      "\n",
      "   WSM_Scenario_1_equal  WSM_Scenario_2_data_code  WSM_Scenario_3_read_train  \n",
      "0              0.285714                  0.206897                   0.446429  \n",
      "1              0.428571                  0.448276                   0.500000  \n",
      "2              0.452381                  0.419540                   0.589286  \n",
      "3              0.642857                  0.603448                   0.642857  \n",
      "4              0.523810                  0.609195                   0.553571  \n",
      "5              0.642857                  0.741379                   0.714286  \n",
      "6              0.595238                  0.591954                   0.750000  \n",
      "7              0.785714                  0.844828                   0.678571  \n",
      "8              0.523810                  0.471264                   0.714286  \n",
      "9              0.595238                  0.660920                   0.696429  \n"
     ]
    }
   ],
   "source": [
    "#Randomly sample 10 papers from the new annotated dataset. it was chosen to sample 10 research papers that were reproducible, \n",
    "#according to the reproducibility factor from D. Olszewski, et al. \\cite{olszewski2023get}, because this subset allows for observing \n",
    "#a wider range of variability in reproducibility scores under different weight assignment procedure specifications.\n",
    "\n",
    "sample_df = (df_norm[df_norm[annotated_col] == 1].sample(n=10, random_state=42).reset_index(drop=True))\n",
    "\n",
    "#Save as a csv file\n",
    "sample_df.to_csv(\"10_annotated_papers.csv\", index=False)\n",
    "\n",
    "#Display results\n",
    "cols = [\"Paper number\", \"Paper Title\"] + [f\"WSM_{k}\" for k in weights] #WSM score for each weight scenario with Paper number and title\n",
    "print(sample_df[cols])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
